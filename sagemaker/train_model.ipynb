{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 1: Imports & Configuration\n",
        "\n",
        "Set up libraries, boto3/sagemaker clients, and variables used by all later cells (edit the bucket name accordingly)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3, io, pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sagemaker\n",
        "from sagemaker import get_execution_role\n",
        "\n",
        "region = \"us-east-1\"  # update if needed\n",
        "bucket = \"healthcare-project-data-jayesh-devre\"\n",
        "role = get_execution_role()\n",
        "s3 = boto3.client(\"s3\", region_name=region)\n",
        "\n",
        "hist_key = \"raw/historical/heart_attack_prediction_dataset.csv\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 2: Load Processed Dataset & Preview\n",
        "\n",
        "Load the processed/merged CSV from S3 (produced by EMR) to confirm shape & columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "obj = s3.get_object(Bucket=bucket, Key=hist_key)\n",
        "df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
        "\n",
        "print(\"Loaded dataset:\", df.shape)\n",
        "df.head(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 3: Function to Preprocess the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_health_data(df):\n",
        "    # Split blood pressure\n",
        "    if \"Blood Pressure\" in df.columns:\n",
        "        bp = df[\"Blood Pressure\"].astype(str).str.split(\"/\", n=1, expand=True)\n",
        "        df[\"BP_Systolic\"] = pd.to_numeric(bp[0], errors=\"coerce\")\n",
        "        df[\"BP_Diastolic\"] = pd.to_numeric(bp[1], errors=\"coerce\")\n",
        "        df.drop(columns=[\"Blood Pressure\"], inplace=True)\n",
        "    \n",
        "    # Drop identifiers\n",
        "    df = df.drop(columns=[\"Patient ID\", \"Country\", \"Continent\", \"Hemisphere\"], errors=\"ignore\")\n",
        "    \n",
        "    # One-hot encode categoricals\n",
        "    df = pd.get_dummies(df, drop_first=True).fillna(0)\n",
        "    \n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 4: Preprocessing and then splitting dataset into train/test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "proc_df = preprocess_health_data(df)\n",
        "y = proc_df[\"Heart Attack Risk\"].astype(int)\n",
        "X = proc_df.drop(columns=[\"Heart Attack Risk\"])\n",
        "\n",
        "final_df = pd.concat([y, X], axis=1)\n",
        "train_df, test_df = train_test_split(final_df, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Train:\", train_df.shape, \"| Test:\", test_df.shape)\n",
        "print(\"\\n Sample training row:\")\n",
        "display(train_df.head(1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 5: Upload the train/test dataset into CSV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_key = \"raw/historical/train/train.csv\"\n",
        "test_key = \"raw/historical/test/test.csv\"\n",
        "\n",
        "def upload_csv(df, key):\n",
        "    s3.put_object(Bucket=bucket, Key=key, Body=df.to_csv(index=False, header=False).encode())\n",
        "    print(f\"Uploaded → s3://{bucket}/{key}\")\n",
        "\n",
        "upload_csv(train_df, train_key)\n",
        "upload_csv(test_df, test_key)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 6: Upload feature list to S3\n",
        "\n",
        "Upload feature list to S3 so that this can be used by the simulated Processed data use the trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_list = list(X.columns)\n",
        "with open(\"feature_list.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(feature_list))\n",
        "\n",
        "!aws s3 cp feature_list.txt s3://{bucket}/preprocess/feature_list.txt\n",
        "print(f\" Uploaded feature list → s3://{bucket}/preprocess/feature_list.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 7: Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sagemaker.estimator import Estimator\n",
        "from sagemaker.inputs import TrainingInput\n",
        "import time\n",
        "\n",
        "timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "output_path = f\"s3://{bucket}/models/xgboost\"\n",
        "\n",
        "xgb_image = sagemaker.image_uris.retrieve(\"xgboost\", region=region, version=\"1.5-1\")\n",
        "\n",
        "xgb_estimator = Estimator(\n",
        "    image_uri=xgb_image,\n",
        "    role=role,\n",
        "    instance_count=1,\n",
        "    instance_type=\"ml.m5.large\",\n",
        "    volume_size=5,\n",
        "    output_path=output_path,\n",
        "    base_job_name=f\"xgboost-heart-attack-{timestamp}\",\n",
        ")\n",
        "\n",
        "xgb_estimator.set_hyperparameters(\n",
        "    objective=\"binary:logistic\",\n",
        "    num_round=100,\n",
        "    eta=0.1,\n",
        "    max_depth=5,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    eval_metric=\"auc\"\n",
        ")\n",
        "\n",
        "train_input = f\"s3://{bucket}/{train_key}\"\n",
        "test_input = f\"s3://{bucket}/{test_key}\"\n",
        "\n",
        "print(\" Starting training job.\")\n",
        "xgb_estimator.fit(\n",
        "    {\n",
        "        \"train\": TrainingInput(train_input, content_type=\"text/csv\"),\n",
        "        \"validation\": TrainingInput(test_input, content_type=\"text/csv\")\n",
        "    }\n",
        ")\n",
        "\n",
        "model_artifact = xgb_estimator.model_data\n",
        "print(\" Model training complete!\")\n",
        "print(\" Model artifact stored at:\", model_artifact)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 8: Deploy the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sagemaker.model import Model\n",
        "import sagemaker, time\n",
        "\n",
        "sagemaker_session = sagemaker.session.Session()\n",
        "timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "\n",
        "xgb_image = sagemaker.image_uris.retrieve(\"xgboost\", region=region, version=\"1.5-1\")\n",
        "\n",
        "# Define the model object\n",
        "xgb_model = Model(\n",
        "    image_uri=xgb_image,\n",
        "    model_data=model_artifact,\n",
        "    role=role,\n",
        "    name=f\"xgb-heart-attack-{timestamp}\",\n",
        "    sagemaker_session=sagemaker_session,\n",
        ")\n",
        "\n",
        "# Create a custom endpoint name\n",
        "endpoint_name = f\"xgb-heart-attack-endpoint-{timestamp}\"\n",
        "\n",
        "print(f\" Deploying XGBoost model as endpoint: {endpoint_name} .\")\n",
        "\n",
        "# Deploy using the model's .deploy() — returns None in newer SDKs,\n",
        "# so we attach a Predictor manually afterward\n",
        "xgb_model.deploy(\n",
        "    initial_instance_count=1,\n",
        "    instance_type=\"ml.m5.large\",\n",
        "    endpoint_name=endpoint_name\n",
        ")\n",
        "\n",
        "# Manually create predictor for runtime access\n",
        "from sagemaker.predictor import Predictor\n",
        "predictor = Predictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n",
        "\n",
        "print(\"\\n Model deployed successfully!\")\n",
        "print(\" Endpoint name:\", endpoint_name)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
